{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dyush\\AppData\\Local\\Temp/ipykernel_276/3233672325.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv('nasdaq_labeled_news.csv', error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('nasdaq_labeled_news.csv', error_bad_lines=False)\n",
    "data_text = data[['News']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Canadian judge on Monday rejected United Sta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TSMC (2330.TW), the world largest contract chi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taiwan UMC (2303.TW), the world second-largest...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australian law firm Slater  Gordon is looking ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Tower (AMT.N) is one of the three fir...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News  index\n",
       "0  A Canadian judge on Monday rejected United Sta...      0\n",
       "1  TSMC (2330.TW), the world largest contract chi...      1\n",
       "2  Taiwan UMC (2303.TW), the world second-largest...      2\n",
       "3  Australian law firm Slater  Gordon is looking ...      3\n",
       "4  American Tower (AMT.N) is one of the three fir...      4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dyush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['A', 'federal', 'judge', 'on', 'Monday', 'declined', 'to', 'approve', 'Citigroup', 'Inc', '(C.N)', '$75', 'million', 'settlement', 'with', 'the', 'U.S.', 'Securities', 'and', 'Exchange', 'Commission', 'of', 'charges', 'it', 'misled', 'investors', 'by', 'failing', 'to', 'disclose', 'roughly', '$40', 'billion', 'of', 'subprime', 'mortgages.', 'U.S.', 'District', 'Judge', 'Ellen', 'Huvelle', 'at', 'a', 'hearing', 'asked', 'both', 'sides', 'for', 'more', 'information', 'on', 'the', 'accord', 'before', 'approving', 'it.', 'Another', 'hearing', 'on', 'the', 'matter', 'is', 'expected', 'on', 'Sept.', '24.', '\"It', 'a', 'home', 'run', 'for', 'the', 'good', 'guys,\"', 'said', 'Richard', 'Greenfield,', 'a', 'lawyer', 'for', 'a', 'Citigroup', 'shareholder,', 'who', 'said', 'he', 'attended', 'the', 'hearing.', '\"The', 'bottom', 'line', 'is', 'that', 'the', 'SEC', 'put', 'virtually', 'nothing', 'forward', 'to', 'justify', 'the', 'settlement.\"', 'SEC', 'spokesman', 'Kevin', 'Callahan', 'said', 'the', 'agency', 'will', 'provide', 'the', 'court', 'with', 'the', 'information', 'requested.', 'Citigroup', 'spokeswoman', 'Molly', 'Meiners', 'said', 'the', 'New', 'York-based', 'bank', '\"will', 'answer', 'all', 'the', 'judge', 'questions', 'concerning', 'this', 'matter.\"', 'In', 'announcing', 'the', 'proposed', 'settlement', 'on', 'July', '29,', 'the', 'SEC', 'said', 'Citigroup', 'understated', 'its', 'exposure', 'to', 'subprime', 'mortgages', 'by', 'about', '$40', 'billion', 'in', '2007,', 'when', 'the', 'nation', 'housing', 'crisis', 'was', 'in', 'its', 'early', 'stages.', 'The', 'SEC', 'also', 'sued', 'Citigroup', 'then-Chief', 'Financial', 'Officer', 'Gary', 'Crittenden', 'and', 'then-investor', 'relations', 'chief', 'Art', 'Tildesley', 'over', 'the', 'matter.', 'They', 'agreed', 'respectively', 'to', 'pay', '$100,000', 'and', '$80,000', 'to', 'settle.', 'None', 'of', 'the', 'defendants', 'admitted', 'wrongdoing.', 'Huvelle', 'decision', 'recalls', 'U.S.', 'District', 'Judge', 'Jed', 'Rakoff', 'rejection', 'last', 'year', 'of', 'the', 'SEC', '$33', 'million', 'accord', 'with', 'Bank', 'of', 'America', 'Corp', '(BAC.N)', 'over', 'that', 'bank', 'alleged', 'misleading', 'of', 'investors', 'about', 'its', 'takeover', 'of', 'Merrill', 'Lynch', '', 'Co.', 'Rakoff', 'later', 'approved', 'a', '$150', 'million', 'settlement.', '\"Well', 'have', 'to', 'see', 'where', 'the', 'judge', 'goes,\"', 'Greenfield', 'said,', 'referring', 'to', 'Huvelle.', '\"Citigroup', 'shareholders', 'have', 'already', 'been', 'victimized,', 'and', 'we', 'dont', 'want', 'that', 'to', 'happen', 'a', 'second', 'time.\"', 'Citigroup', 'exposure', 'to', 'risky', 'debt', 'led', 'to', 'a', 'series', 'of', 'federal', 'bailouts', 'that', 'left', 'taxpayers', 'owning', 'about', 'one-third', 'of', 'the', 'bank.', 'The', 'government', 'has', 'since', 'been', 'reducing', 'that', 'stake.', 'Shares', 'of', 'Citigroup', 'closed', 'Monday', 'down', '1', 'cent', 'at', '$3.87.', 'The', 'case', 'is', 'SEC', 'v.', 'Citigroup', 'Inc,', 'U.S.', 'District', 'Court,', 'District', 'of', 'Columbia,', 'No.', '10-01277.', '(Reporting', 'by', 'Jonathan', 'Stempel', 'and', 'Dan', 'Wilchins;editing', 'by', 'Sofina', 'Mirza-Reid)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['feder', 'judg', 'monday', 'declin', 'approv', 'citigroup', 'million', 'settlement', 'secur', 'exchang', 'commiss', 'charg', 'mislead', 'investor', 'fail', 'disclos', 'rough', 'billion', 'subprim', 'mortgag', 'district', 'judg', 'ellen', 'huvell', 'hear', 'ask', 'side', 'inform', 'accord', 'approv', 'hear', 'matter', 'expect', 'sept', 'home', 'good', 'guy', 'say', 'richard', 'greenfield', 'lawyer', 'citigroup', 'sharehold', 'say', 'attend', 'hear', 'line', 'virtual', 'forward', 'justifi', 'settlement', 'spokesman', 'kevin', 'callahan', 'say', 'agenc', 'provid', 'court', 'inform', 'request', 'citigroup', 'spokeswoman', 'molli', 'meiner', 'say', 'york', 'base', 'bank', 'answer', 'judg', 'question', 'concern', 'matter', 'announc', 'propos', 'settlement', 'juli', 'say', 'citigroup', 'underst', 'exposur', 'subprim', 'mortgag', 'billion', 'nation', 'hous', 'crisi', 'earli', 'stag', 'sue', 'citigroup', 'chief', 'financi', 'offic', 'gari', 'crittenden', 'investor', 'relat', 'chief', 'tildesley', 'matter', 'agre', 'respect', 'settl', 'defend', 'admit', 'wrongdo', 'huvell', 'decis', 'recal', 'district', 'judg', 'rakoff', 'reject', 'year', 'million', 'accord', 'bank', 'america', 'corp', 'bank', 'alleg', 'mislead', 'investor', 'takeov', 'merril', 'lynch', 'rakoff', 'later', 'approv', 'million', 'settlement', 'judg', 'go', 'greenfield', 'say', 'refer', 'huvell', 'citigroup', 'sharehold', 'victim', 'dont', 'want', 'happen', 'second', 'time', 'citigroup', 'exposur', 'riski', 'debt', 'seri', 'feder', 'bailout', 'leav', 'taxpay', 'own', 'bank', 'govern', 'reduc', 'stake', 'share', 'citigroup', 'close', 'monday', 'cent', 'case', 'citigroup', 'district', 'court', 'district', 'columbia', 'report', 'jonathan', 'stempel', 'wilchin', 'edit', 'sofina', 'mirza', 'reid']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['News'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [canadian, judg, monday, reject, unit, state, ...\n",
       "1    [tsmc, world, largest, contract, chip, maker, ...\n",
       "2    [taiwan, world, second, largest, contract, chi...\n",
       "3    [australian, firm, slater, gordon, look, possi...\n",
       "4    [american, tower, firm, talk, telecom, tower, ...\n",
       "5    [american, tower, firm, talk, telecom, tower, ...\n",
       "6    [australia, propos, percent, mine, hurt, cater...\n",
       "7    [telecom, media, tycoon, richard, privat, equi...\n",
       "8    [collaps, commerci, real, estat, bubbl, crush,...\n",
       "9    [follow, main, factor, expect, affect, swiss, ...\n",
       "Name: News, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 affect\n",
      "1 allan\n",
      "2 alleg\n",
      "3 approv\n",
      "4 argu\n",
      "5 author\n",
      "6 base\n",
      "7 benefit\n",
      "8 blame\n",
      "9 block\n",
      "10 break\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (2, 3),\n",
       " (5, 1),\n",
       " (17, 1),\n",
       " (20, 2),\n",
       " (22, 1),\n",
       " (31, 2),\n",
       " (34, 1),\n",
       " (36, 1),\n",
       " (42, 5),\n",
       " (46, 1),\n",
       " (51, 2),\n",
       " (53, 1),\n",
       " (69, 1),\n",
       " (81, 1),\n",
       " (83, 1),\n",
       " (86, 1),\n",
       " (108, 2),\n",
       " (117, 1),\n",
       " (125, 1),\n",
       " (145, 1),\n",
       " (148, 2),\n",
       " (164, 1),\n",
       " (186, 2),\n",
       " (193, 1),\n",
       " (196, 2),\n",
       " (197, 1),\n",
       " (203, 3),\n",
       " (209, 3),\n",
       " (217, 3),\n",
       " (229, 4),\n",
       " (279, 1),\n",
       " (286, 1),\n",
       " (296, 1),\n",
       " (302, 1),\n",
       " (334, 1),\n",
       " (338, 1),\n",
       " (341, 4),\n",
       " (366, 1),\n",
       " (374, 1),\n",
       " (379, 1),\n",
       " (383, 1),\n",
       " (388, 1),\n",
       " (391, 2),\n",
       " (413, 1),\n",
       " (426, 2),\n",
       " (428, 1),\n",
       " (440, 1),\n",
       " (454, 1),\n",
       " (462, 1),\n",
       " (513, 3),\n",
       " (526, 1),\n",
       " (538, 1),\n",
       " (549, 9),\n",
       " (579, 1),\n",
       " (595, 1),\n",
       " (598, 1),\n",
       " (610, 1),\n",
       " (612, 1),\n",
       " (624, 1),\n",
       " (647, 1),\n",
       " (697, 1),\n",
       " (725, 1),\n",
       " (745, 1),\n",
       " (776, 1),\n",
       " (779, 1),\n",
       " (857, 1),\n",
       " (863, 1),\n",
       " (899, 1),\n",
       " (909, 1),\n",
       " (926, 2),\n",
       " (964, 1),\n",
       " (966, 1),\n",
       " (980, 1),\n",
       " (993, 1),\n",
       " (1044, 4),\n",
       " (1084, 1),\n",
       " (1112, 1),\n",
       " (1114, 1),\n",
       " (1208, 1),\n",
       " (1252, 1),\n",
       " (1257, 1),\n",
       " (1370, 1),\n",
       " (1404, 1),\n",
       " (1410, 1),\n",
       " (1431, 1),\n",
       " (1436, 1),\n",
       " (1441, 1),\n",
       " (1519, 1),\n",
       " (1542, 1),\n",
       " (1610, 1),\n",
       " (1613, 1),\n",
       " (1616, 1),\n",
       " (1617, 1),\n",
       " (1665, 1),\n",
       " (1772, 1),\n",
       " (1778, 2),\n",
       " (2012, 1),\n",
       " (2020, 1),\n",
       " (2051, 1),\n",
       " (2052, 1),\n",
       " (2094, 1),\n",
       " (2102, 1),\n",
       " (2143, 2),\n",
       " (2297, 1),\n",
       " (2439, 1),\n",
       " (2481, 1),\n",
       " (2658, 1),\n",
       " (2876, 1),\n",
       " (2951, 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1 (\"alleg\") appears 1 time.\n",
      "Word 2 (\"approv\") appears 3 time.\n",
      "Word 5 (\"base\") appears 1 time.\n",
      "Word 17 (\"corp\") appears 1 time.\n",
      "Word 20 (\"court\") appears 2 time.\n",
      "Word 22 (\"decis\") appears 1 time.\n",
      "Word 31 (\"feder\") appears 2 time.\n",
      "Word 34 (\"gari\") appears 1 time.\n",
      "Word 36 (\"govern\") appears 1 time.\n",
      "Word 42 (\"judg\") appears 5 time.\n",
      "Word 46 (\"lawyer\") appears 1 time.\n",
      "Word 51 (\"monday\") appears 2 time.\n",
      "Word 53 (\"nation\") appears 1 time.\n",
      "Word 69 (\"reject\") appears 1 time.\n",
      "Word 81 (\"sue\") appears 1 time.\n",
      "Word 83 (\"takeov\") appears 1 time.\n",
      "Word 86 (\"time\") appears 1 time.\n",
      "Word 108 (\"chief\") appears 2 time.\n",
      "Word 117 (\"financi\") appears 1 time.\n",
      "Word 125 (\"jonathan\") appears 1 time.\n",
      "Word 145 (\"second\") appears 1 time.\n",
      "Word 148 (\"sharehold\") appears 2 time.\n",
      "Word 164 (\"exchang\") appears 1 time.\n",
      "Word 186 (\"billion\") appears 2 time.\n",
      "Word 193 (\"debt\") appears 1 time.\n",
      "Word 196 (\"exposur\") appears 2 time.\n",
      "Word 197 (\"fail\") appears 1 time.\n",
      "Word 203 (\"hear\") appears 3 time.\n",
      "Word 209 (\"investor\") appears 3 time.\n",
      "Word 217 (\"million\") appears 3 time.\n",
      "Word 229 (\"settlement\") appears 4 time.\n",
      "Word 279 (\"propos\") appears 1 time.\n",
      "Word 286 (\"stake\") appears 1 time.\n",
      "Word 296 (\"agenc\") appears 1 time.\n",
      "Word 302 (\"concern\") appears 1 time.\n",
      "Word 334 (\"spokeswoman\") appears 1 time.\n",
      "Word 338 (\"ask\") appears 1 time.\n",
      "Word 341 (\"bank\") appears 4 time.\n",
      "Word 366 (\"line\") appears 1 time.\n",
      "Word 374 (\"own\") appears 1 time.\n",
      "Word 379 (\"richard\") appears 1 time.\n",
      "Word 383 (\"share\") appears 1 time.\n",
      "Word 388 (\"want\") appears 1 time.\n",
      "Word 391 (\"accord\") appears 2 time.\n",
      "Word 413 (\"good\") appears 1 time.\n",
      "Word 426 (\"mortgag\") appears 2 time.\n",
      "Word 428 (\"offic\") appears 1 time.\n",
      "Word 440 (\"relat\") appears 1 time.\n",
      "Word 454 (\"york\") appears 1 time.\n",
      "Word 462 (\"expect\") appears 1 time.\n",
      "Word 513 (\"matter\") appears 3 time.\n",
      "Word 526 (\"charg\") appears 1 time.\n",
      "Word 538 (\"spokesman\") appears 1 time.\n",
      "Word 549 (\"citigroup\") appears 9 time.\n",
      "Word 579 (\"close\") appears 1 time.\n",
      "Word 595 (\"earli\") appears 1 time.\n",
      "Word 598 (\"forward\") appears 1 time.\n",
      "Word 610 (\"provid\") appears 1 time.\n",
      "Word 612 (\"secur\") appears 1 time.\n",
      "Word 624 (\"declin\") appears 1 time.\n",
      "Word 647 (\"respect\") appears 1 time.\n",
      "Word 697 (\"dont\") appears 1 time.\n",
      "Word 725 (\"america\") appears 1 time.\n",
      "Word 745 (\"later\") appears 1 time.\n",
      "Word 776 (\"agre\") appears 1 time.\n",
      "Word 779 (\"case\") appears 1 time.\n",
      "Word 857 (\"hous\") appears 1 time.\n",
      "Word 863 (\"reduc\") appears 1 time.\n",
      "Word 899 (\"refer\") appears 1 time.\n",
      "Word 909 (\"cent\") appears 1 time.\n",
      "Word 926 (\"inform\") appears 2 time.\n",
      "Word 964 (\"commiss\") appears 1 time.\n",
      "Word 966 (\"disclos\") appears 1 time.\n",
      "Word 980 (\"leav\") appears 1 time.\n",
      "Word 993 (\"announc\") appears 1 time.\n",
      "Word 1044 (\"district\") appears 4 time.\n",
      "Word 1084 (\"recal\") appears 1 time.\n",
      "Word 1112 (\"lynch\") appears 1 time.\n",
      "Word 1114 (\"merril\") appears 1 time.\n",
      "Word 1208 (\"crisi\") appears 1 time.\n",
      "Word 1252 (\"home\") appears 1 time.\n",
      "Word 1257 (\"rough\") appears 1 time.\n",
      "Word 1370 (\"defend\") appears 1 time.\n",
      "Word 1404 (\"go\") appears 1 time.\n",
      "Word 1410 (\"question\") appears 1 time.\n",
      "Word 1431 (\"juli\") appears 1 time.\n",
      "Word 1436 (\"request\") appears 1 time.\n",
      "Word 1441 (\"sept\") appears 1 time.\n",
      "Word 1519 (\"happen\") appears 1 time.\n",
      "Word 1542 (\"side\") appears 1 time.\n",
      "Word 1610 (\"mirza\") appears 1 time.\n",
      "Word 1613 (\"reid\") appears 1 time.\n",
      "Word 1616 (\"seri\") appears 1 time.\n",
      "Word 1617 (\"sofina\") appears 1 time.\n",
      "Word 1665 (\"kevin\") appears 1 time.\n",
      "Word 1772 (\"attend\") appears 1 time.\n",
      "Word 1778 (\"subprim\") appears 2 time.\n",
      "Word 2012 (\"bailout\") appears 1 time.\n",
      "Word 2020 (\"victim\") appears 1 time.\n",
      "Word 2051 (\"settl\") appears 1 time.\n",
      "Word 2052 (\"stempel\") appears 1 time.\n",
      "Word 2094 (\"stag\") appears 1 time.\n",
      "Word 2102 (\"taxpay\") appears 1 time.\n",
      "Word 2143 (\"mislead\") appears 2 time.\n",
      "Word 2297 (\"justifi\") appears 1 time.\n",
      "Word 2439 (\"riski\") appears 1 time.\n",
      "Word 2481 (\"wilchin\") appears 1 time.\n",
      "Word 2658 (\"admit\") appears 1 time.\n",
      "Word 2876 (\"columbia\") appears 1 time.\n",
      "Word 2951 (\"virtual\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.04740335979405965),\n",
      " (1, 0.05981785536616545),\n",
      " (2, 0.07933472162027505),\n",
      " (3, 0.06999493037504778),\n",
      " (4, 0.047814003040763386),\n",
      " (5, 0.02032364784655052),\n",
      " (6, 0.09704768233305229),\n",
      " (7, 0.08004216311084526),\n",
      " (8, 0.05548383532177425),\n",
      " (9, 0.10750941390799126),\n",
      " (10, 0.10209032644900137),\n",
      " (11, 0.2768492942605557),\n",
      " (12, 0.037153576412417455),\n",
      " (13, 0.09899735559954259),\n",
      " (14, 0.057175445056947494),\n",
      " (15, 0.08318161323828102),\n",
      " (16, 0.07363838083901733),\n",
      " (17, 0.017601985012046104),\n",
      " (18, 0.03187734923367338),\n",
      " (19, 0.0786892340082342),\n",
      " (20, 0.1388888150158044),\n",
      " (21, 0.053960478695264526),\n",
      " (22, 0.18248120320221692),\n",
      " (23, 0.03334333815919392),\n",
      " (24, 0.06522068739286731),\n",
      " (25, 0.044131916726854147),\n",
      " (26, 0.05951726211655632),\n",
      " (27, 0.06479483321574765),\n",
      " (28, 0.03898947143996813),\n",
      " (29, 0.050293143888028656),\n",
      " (30, 0.068587542260898),\n",
      " (31, 0.07917207121704016),\n",
      " (32, 0.14881005762315033),\n",
      " (33, 0.1464517282369123),\n",
      " (34, 0.05981785536616545),\n",
      " (35, 0.07845063724685906),\n",
      " (36, 0.2055988395628407),\n",
      " (37, 0.07255583016038988),\n",
      " (38, 0.058790596955063015),\n",
      " (39, 0.16244391550228324),\n",
      " (40, 0.052848659927863674),\n",
      " (41, 0.16928250998332484),\n",
      " (42, 0.1098444853711206),\n",
      " (43, 0.068587542260898),\n",
      " (44, 0.0806207134529389),\n",
      " (45, 0.05113104448302213),\n",
      " (46, 0.0665741922720959),\n",
      " (47, 0.06832093093997547),\n",
      " (48, 0.039997045872582654),\n",
      " (49, 0.059666821981397215),\n",
      " (50, 0.053960478695264526),\n",
      " (51, 0.025507104765389595),\n",
      " (52, 0.0769659285969173),\n",
      " (53, 0.041075532995307695),\n",
      " (54, 0.1568540224254595),\n",
      " (55, 0.0519264267930502),\n",
      " (56, 0.08122195775114162),\n",
      " (57, 0.23528103363818928),\n",
      " (58, 0.08389475412492095),\n",
      " (59, 0.04726888731290933),\n",
      " (60, 0.04457468438714111),\n",
      " (61, 0.05130426400460764),\n",
      " (62, 0.21087781517786508),\n",
      " (63, 0.06108288322989548),\n",
      " (64, 0.11028883415031918),\n",
      " (65, 0.13087626901510113),\n",
      " (66, 0.04318010844636733),\n",
      " (67, 0.07606391758544168),\n",
      " (68, 0.08250017818414716),\n",
      " (69, 0.11701861795909381),\n",
      " (70, 0.09521461290944035),\n",
      " (71, 0.03557653672967754),\n",
      " (72, 0.04532262611631366),\n",
      " (73, 0.04116518099671263),\n",
      " (74, 0.05345113734003348),\n",
      " (75, 0.07090587741974176),\n",
      " (76, 0.055830939159223),\n",
      " (77, 0.02435371282627251),\n",
      " (78, 0.05546643333577248),\n",
      " (79, 0.35533469919712096),\n",
      " (80, 0.05981785536616545),\n",
      " (81, 0.06565871046489229),\n",
      " (82, 0.03532877756619922),\n",
      " (83, 0.16314149555560684),\n",
      " (84, 0.07563206467431788),\n",
      " (85, 0.03601999059249603),\n",
      " (86, 0.0260844099493335),\n",
      " (87, 0.08250017818414716),\n",
      " (88, 0.16035341202010045),\n",
      " (89, 0.03822876301077894),\n",
      " (90, 0.07792435094915033),\n",
      " (91, 0.13998986075009556),\n",
      " (92, 0.08542900998015286),\n",
      " (93, 0.19697613139467685),\n",
      " (94, 0.07480322690226175),\n",
      " (95, 0.04407745924666492),\n",
      " (96, 0.06832093093997547),\n",
      " (97, 0.05121740958217209),\n",
      " (98, 0.03954553422551051)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.035*\"govern\" + 0.024*\"open\" + 0.018*\"coast\" + 0.017*\"tasmanian\" + 0.017*\"gold\" + 0.014*\"australia\" + 0.013*\"beat\" + 0.010*\"win\" + 0.010*\"ahead\" + 0.009*\"shark\"\n",
      "Topic: 1 \n",
      "Words: 0.023*\"world\" + 0.014*\"final\" + 0.013*\"record\" + 0.012*\"break\" + 0.011*\"lose\" + 0.011*\"australian\" + 0.011*\"leagu\" + 0.011*\"test\" + 0.010*\"australia\" + 0.010*\"hill\"\n",
      "Topic: 2 \n",
      "Words: 0.018*\"rural\" + 0.018*\"council\" + 0.015*\"fund\" + 0.014*\"plan\" + 0.013*\"health\" + 0.012*\"chang\" + 0.011*\"nation\" + 0.010*\"price\" + 0.010*\"servic\" + 0.009*\"say\"\n",
      "Topic: 3 \n",
      "Words: 0.025*\"elect\" + 0.022*\"adelaid\" + 0.012*\"perth\" + 0.011*\"take\" + 0.011*\"say\" + 0.010*\"labor\" + 0.010*\"turnbul\" + 0.009*\"vote\" + 0.009*\"royal\" + 0.009*\"time\"\n",
      "Topic: 4 \n",
      "Words: 0.032*\"court\" + 0.022*\"face\" + 0.020*\"charg\" + 0.020*\"home\" + 0.018*\"tasmania\" + 0.017*\"murder\" + 0.015*\"trial\" + 0.012*\"accus\" + 0.012*\"abus\" + 0.012*\"child\"\n",
      "Topic: 5 \n",
      "Words: 0.024*\"countri\" + 0.021*\"hour\" + 0.020*\"australian\" + 0.019*\"warn\" + 0.016*\"live\" + 0.013*\"indigen\" + 0.011*\"call\" + 0.009*\"victorian\" + 0.009*\"campaign\" + 0.008*\"show\"\n",
      "Topic: 6 \n",
      "Words: 0.027*\"south\" + 0.024*\"year\" + 0.020*\"interview\" + 0.020*\"north\" + 0.019*\"jail\" + 0.018*\"west\" + 0.014*\"island\" + 0.013*\"australia\" + 0.013*\"victoria\" + 0.010*\"china\"\n",
      "Topic: 7 \n",
      "Words: 0.031*\"queensland\" + 0.029*\"melbourn\" + 0.018*\"water\" + 0.017*\"claim\" + 0.013*\"hunter\" + 0.012*\"green\" + 0.012*\"resid\" + 0.011*\"darwin\" + 0.010*\"young\" + 0.009*\"plead\"\n",
      "Topic: 8 \n",
      "Words: 0.017*\"attack\" + 0.016*\"kill\" + 0.012*\"victim\" + 0.012*\"violenc\" + 0.010*\"hobart\" + 0.010*\"rugbi\" + 0.010*\"secur\" + 0.010*\"say\" + 0.009*\"state\" + 0.008*\"domest\"\n",
      "Topic: 9 \n",
      "Words: 0.052*\"polic\" + 0.020*\"crash\" + 0.019*\"death\" + 0.017*\"sydney\" + 0.016*\"miss\" + 0.016*\"woman\" + 0.015*\"die\" + 0.015*\"charg\" + 0.014*\"shoot\" + 0.013*\"arrest\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.006*\"share\" + 0.006*\"cent\" + 0.005*\"million\" + 0.005*\"quarter\" + 0.004*\"refineri\" + 0.004*\"revenu\" + 0.003*\"earn\" + 0.003*\"barrel\" + 0.003*\"expect\" + 0.003*\"gulf\"\n",
      "Topic: 1 Word: 0.005*\"bank\" + 0.003*\"note\" + 0.003*\"billion\" + 0.003*\"sale\" + 0.003*\"share\" + 0.003*\"million\" + 0.003*\"sourc\" + 0.003*\"quarter\" + 0.003*\"gulf\" + 0.003*\"spill\"\n",
      "Topic: 2 Word: 0.004*\"share\" + 0.004*\"ford\" + 0.004*\"bank\" + 0.003*\"quarter\" + 0.003*\"sale\" + 0.003*\"million\" + 0.003*\"cent\" + 0.003*\"billion\" + 0.003*\"fund\" + 0.003*\"profit\"\n",
      "Topic: 3 Word: 0.008*\"cent\" + 0.007*\"share\" + 0.007*\"quarter\" + 0.005*\"million\" + 0.005*\"revenu\" + 0.005*\"earn\" + 0.005*\"profit\" + 0.004*\"analyst\" + 0.004*\"expect\" + 0.004*\"rise\"\n",
      "Topic: 4 Word: 0.006*\"share\" + 0.005*\"quarter\" + 0.005*\"cent\" + 0.005*\"million\" + 0.004*\"earn\" + 0.004*\"revenu\" + 0.004*\"expect\" + 0.003*\"profit\" + 0.003*\"analyst\" + 0.003*\"billion\"\n",
      "Topic: 5 Word: 0.005*\"share\" + 0.004*\"quarter\" + 0.004*\"cent\" + 0.004*\"million\" + 0.003*\"sale\" + 0.003*\"loss\" + 0.003*\"expect\" + 0.003*\"rise\" + 0.002*\"product\" + 0.002*\"oper\"\n",
      "Topic: 6 Word: 0.004*\"bank\" + 0.004*\"sourc\" + 0.004*\"fund\" + 0.003*\"billion\" + 0.003*\"equiti\" + 0.003*\"deal\" + 0.003*\"group\" + 0.003*\"firm\" + 0.003*\"privat\" + 0.003*\"invest\"\n",
      "Topic: 7 Word: 0.006*\"bank\" + 0.003*\"million\" + 0.003*\"share\" + 0.003*\"billion\" + 0.003*\"manag\" + 0.003*\"quarter\" + 0.003*\"sale\" + 0.003*\"invest\" + 0.003*\"hire\" + 0.002*\"asset\"\n",
      "Topic: 8 Word: 0.005*\"index\" + 0.004*\"point\" + 0.004*\"bank\" + 0.003*\"share\" + 0.003*\"sourc\" + 0.003*\"china\" + 0.002*\"billion\" + 0.002*\"goldman\" + 0.002*\"group\" + 0.002*\"european\"\n",
      "Topic: 9 Word: 0.005*\"bank\" + 0.004*\"share\" + 0.003*\"quarter\" + 0.003*\"vlastelica\" + 0.003*\"million\" + 0.003*\"rise\" + 0.003*\"billion\" + 0.003*\"premarket\" + 0.003*\"ryan\" + 0.003*\"earn\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feder',\n",
       " 'judg',\n",
       " 'monday',\n",
       " 'declin',\n",
       " 'approv',\n",
       " 'citigroup',\n",
       " 'million',\n",
       " 'settlement',\n",
       " 'secur',\n",
       " 'exchang',\n",
       " 'commiss',\n",
       " 'charg',\n",
       " 'mislead',\n",
       " 'investor',\n",
       " 'fail',\n",
       " 'disclos',\n",
       " 'rough',\n",
       " 'billion',\n",
       " 'subprim',\n",
       " 'mortgag',\n",
       " 'district',\n",
       " 'judg',\n",
       " 'ellen',\n",
       " 'huvell',\n",
       " 'hear',\n",
       " 'ask',\n",
       " 'side',\n",
       " 'inform',\n",
       " 'accord',\n",
       " 'approv',\n",
       " 'hear',\n",
       " 'matter',\n",
       " 'expect',\n",
       " 'sept',\n",
       " 'home',\n",
       " 'good',\n",
       " 'guy',\n",
       " 'say',\n",
       " 'richard',\n",
       " 'greenfield',\n",
       " 'lawyer',\n",
       " 'citigroup',\n",
       " 'sharehold',\n",
       " 'say',\n",
       " 'attend',\n",
       " 'hear',\n",
       " 'line',\n",
       " 'virtual',\n",
       " 'forward',\n",
       " 'justifi',\n",
       " 'settlement',\n",
       " 'spokesman',\n",
       " 'kevin',\n",
       " 'callahan',\n",
       " 'say',\n",
       " 'agenc',\n",
       " 'provid',\n",
       " 'court',\n",
       " 'inform',\n",
       " 'request',\n",
       " 'citigroup',\n",
       " 'spokeswoman',\n",
       " 'molli',\n",
       " 'meiner',\n",
       " 'say',\n",
       " 'york',\n",
       " 'base',\n",
       " 'bank',\n",
       " 'answer',\n",
       " 'judg',\n",
       " 'question',\n",
       " 'concern',\n",
       " 'matter',\n",
       " 'announc',\n",
       " 'propos',\n",
       " 'settlement',\n",
       " 'juli',\n",
       " 'say',\n",
       " 'citigroup',\n",
       " 'underst',\n",
       " 'exposur',\n",
       " 'subprim',\n",
       " 'mortgag',\n",
       " 'billion',\n",
       " 'nation',\n",
       " 'hous',\n",
       " 'crisi',\n",
       " 'earli',\n",
       " 'stag',\n",
       " 'sue',\n",
       " 'citigroup',\n",
       " 'chief',\n",
       " 'financi',\n",
       " 'offic',\n",
       " 'gari',\n",
       " 'crittenden',\n",
       " 'investor',\n",
       " 'relat',\n",
       " 'chief',\n",
       " 'tildesley',\n",
       " 'matter',\n",
       " 'agre',\n",
       " 'respect',\n",
       " 'settl',\n",
       " 'defend',\n",
       " 'admit',\n",
       " 'wrongdo',\n",
       " 'huvell',\n",
       " 'decis',\n",
       " 'recal',\n",
       " 'district',\n",
       " 'judg',\n",
       " 'rakoff',\n",
       " 'reject',\n",
       " 'year',\n",
       " 'million',\n",
       " 'accord',\n",
       " 'bank',\n",
       " 'america',\n",
       " 'corp',\n",
       " 'bank',\n",
       " 'alleg',\n",
       " 'mislead',\n",
       " 'investor',\n",
       " 'takeov',\n",
       " 'merril',\n",
       " 'lynch',\n",
       " 'rakoff',\n",
       " 'later',\n",
       " 'approv',\n",
       " 'million',\n",
       " 'settlement',\n",
       " 'judg',\n",
       " 'go',\n",
       " 'greenfield',\n",
       " 'say',\n",
       " 'refer',\n",
       " 'huvell',\n",
       " 'citigroup',\n",
       " 'sharehold',\n",
       " 'victim',\n",
       " 'dont',\n",
       " 'want',\n",
       " 'happen',\n",
       " 'second',\n",
       " 'time',\n",
       " 'citigroup',\n",
       " 'exposur',\n",
       " 'riski',\n",
       " 'debt',\n",
       " 'seri',\n",
       " 'feder',\n",
       " 'bailout',\n",
       " 'leav',\n",
       " 'taxpay',\n",
       " 'own',\n",
       " 'bank',\n",
       " 'govern',\n",
       " 'reduc',\n",
       " 'stake',\n",
       " 'share',\n",
       " 'citigroup',\n",
       " 'close',\n",
       " 'monday',\n",
       " 'cent',\n",
       " 'case',\n",
       " 'citigroup',\n",
       " 'district',\n",
       " 'court',\n",
       " 'district',\n",
       " 'columbia',\n",
       " 'report',\n",
       " 'jonathan',\n",
       " 'stempel',\n",
       " 'wilchin',\n",
       " 'edit',\n",
       " 'sofina',\n",
       " 'mirza',\n",
       " 'reid']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9921139478683472\t \n",
      "Topic: 0.012*\"million\" + 0.010*\"insur\" + 0.008*\"file\" + 0.008*\"court\" + 0.007*\"york\" + 0.006*\"lilli\" + 0.006*\"financi\" + 0.006*\"accord\" + 0.006*\"group\" + 0.005*\"unit\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.72900390625\t \n",
      "Topic: 0.005*\"index\" + 0.004*\"point\" + 0.004*\"bank\" + 0.003*\"share\" + 0.003*\"sourc\" + 0.003*\"china\" + 0.002*\"billion\" + 0.002*\"goldman\" + 0.002*\"group\" + 0.002*\"european\"\n",
      "\n",
      "Score: 0.1399843394756317\t \n",
      "Topic: 0.004*\"share\" + 0.004*\"ford\" + 0.004*\"bank\" + 0.003*\"quarter\" + 0.003*\"sale\" + 0.003*\"million\" + 0.003*\"cent\" + 0.003*\"billion\" + 0.003*\"fund\" + 0.003*\"profit\"\n",
      "\n",
      "Score: 0.1264623999595642\t \n",
      "Topic: 0.006*\"bank\" + 0.003*\"million\" + 0.003*\"share\" + 0.003*\"billion\" + 0.003*\"manag\" + 0.003*\"quarter\" + 0.003*\"sale\" + 0.003*\"invest\" + 0.003*\"hire\" + 0.002*\"asset\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5905829071998596\t Topic: 0.012*\"billion\" + 0.012*\"bank\" + 0.010*\"million\" + 0.009*\"share\" + 0.009*\"group\"\n",
      "Score: 0.24930903315544128\t Topic: 0.010*\"bank\" + 0.008*\"court\" + 0.007*\"airlin\" + 0.006*\"case\" + 0.006*\"expect\"\n",
      "Score: 0.020018169656395912\t Topic: 0.017*\"bank\" + 0.017*\"billion\" + 0.016*\"million\" + 0.010*\"quarter\" + 0.009*\"share\"\n",
      "Score: 0.020015809684991837\t Topic: 0.006*\"share\" + 0.006*\"drug\" + 0.006*\"state\" + 0.006*\"trade\" + 0.005*\"plan\"\n",
      "Score: 0.020013874396681786\t Topic: 0.010*\"bank\" + 0.009*\"million\" + 0.009*\"billion\" + 0.009*\"group\" + 0.009*\"share\"\n",
      "Score: 0.020013414323329926\t Topic: 0.010*\"billion\" + 0.010*\"share\" + 0.008*\"million\" + 0.008*\"plan\" + 0.007*\"stock\"\n",
      "Score: 0.020012380555272102\t Topic: 0.012*\"million\" + 0.010*\"insur\" + 0.008*\"file\" + 0.008*\"court\" + 0.007*\"york\"\n",
      "Score: 0.020011989399790764\t Topic: 0.009*\"gulf\" + 0.008*\"manag\" + 0.008*\"bank\" + 0.008*\"mexico\" + 0.008*\"oper\"\n",
      "Score: 0.02001143805682659\t Topic: 0.022*\"share\" + 0.017*\"million\" + 0.011*\"quarter\" + 0.010*\"rise\" + 0.009*\"expect\"\n",
      "Score: 0.02001100406050682\t Topic: 0.032*\"share\" + 0.024*\"quarter\" + 0.020*\"million\" + 0.018*\"cent\" + 0.016*\"expect\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
